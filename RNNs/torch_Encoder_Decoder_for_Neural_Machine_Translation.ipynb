{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQKwm5mjwnSJomynP1mnqY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerasole/ml4hep/blob/main/RNNs/torch_Encoder_Decoder_for_Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine translation using RNNs\n",
        "\n",
        "The theory of machine translation using RNNs is described, for instance, in Chapter 16 of https://github.com/ageron/handson-ml3.\n",
        "\n",
        "Basics of the algorithm\n",
        "- Input: sentence in a given language,\n",
        "- Output: translation of the input sentence in a different language.\n",
        "\n",
        "The architecture consists in an encoder-decoder system.\n",
        "- The input sentence is fed to the encoder, which transforms the input into a low-dimensional latent representation\n",
        "- The decoder transforms the latent representation into an output sentence."
      ],
      "metadata": {
        "id": "H8aW01b9m1_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4PVrt2XsjCI-"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ts-ggQo_4G",
        "outputId": "71d0a190-fb66-4742-8f7c-65e5691e05ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language class for tokenization"
      ],
      "metadata": {
        "id": "6u8C4oJaRWk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a class for handling languages and sentences.\n",
        "\n",
        "This class will need to\n",
        "- register the words in the sentences given as input to the class\n",
        "\n",
        "In other words, we employ this class to perform the tokenization = encode every word as an integer.\n",
        "\n",
        "In the preprocessing, we will uniform the sentences to a standard. This is is not included in the Lang class and it is a fundamental preprocessing step."
      ],
      "metadata": {
        "id": "DhJySh2Lr10i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0   # Start-of-Sentence\n",
        "EOS_token = 1   # End-of-Sentence\n",
        "\n",
        "class Lang:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name # name of the language\n",
        "        self.word2index = {} # dictionary containing words: indices when they were first inserted in the dictionary\n",
        "        self.word2count = {} # dictionary containing words: number of times they were inserted in the dictionary\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # dictionary contiaining indices of first insertion in the dictionary: words\n",
        "        self.n_words = 2  # Total counts of words in the dictionary, including SOS and EOS\n",
        "\n",
        "    def addWord(self, word):\n",
        "        # When we add a word, we have to check if it is already present in the dictionary, e.g. in the word2index.\n",
        "        # If it is not present, we need to\n",
        "        #  - add this word to the self.word2index dictionary, giving it as index the current self.n_words,\n",
        "        #  - add this word to the self.word2count dictionary, giving it 1 count,\n",
        "        #  - add, in the self.index2word dictionary, using as index given by self.n_words, the word itself,\n",
        "        #  - increase by 1 the number of total words, aka self.n_words.\n",
        "        # If it is already present, we need to\n",
        "        #  - increase by 1 the corresponding self.word2counts entry.\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        # Add words splitting the sentence by space.\n",
        "        # It would be better if the sentence is already standardized.\n",
        "        # We will take care of this in the preprocessing of the sentences.\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n"
      ],
      "metadata": {
        "id": "4Lq1ePc9pFpF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = Lang(\"prova\")\n",
        "print (lang.word2count, lang.word2index, lang.index2word, lang.n_words)\n",
        "lang.addSentence(\"could you please stop the noise?\")\n",
        "print (lang.word2count, lang.word2index, lang.index2word, lang.n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV19B5uzrHzu",
        "outputId": "70add680-3883-44c5-8478-6653ab042dc4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{} {} {0: 'SOS', 1: 'EOS'} 2\n",
            "{'could': 1, 'you': 1, 'please': 1, 'stop': 1, 'the': 1, 'noise?': 1} {'could': 2, 'you': 3, 'please': 4, 'stop': 5, 'the': 6, 'noise?': 7} {0: 'SOS', 1: 'EOS', 2: 'could', 3: 'you', 4: 'please', 5: 'stop', 6: 'the', 7: 'noise?'} 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: standardization of the sentences"
      ],
      "metadata": {
        "id": "3iGBuGqoRbd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cells, we will investigate methods to standardize the input sentences:\n",
        "- transform to lower case\n",
        "- all special characters need to be treated differently\n",
        "- Take into account abbreviations"
      ],
      "metadata": {
        "id": "TzOUkimX07an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/a/518232/2809427\n",
        "# Turn a Unicode string to plain ASCIIi, i.e.\n",
        "# - remove the accents without changing the letter\n",
        "# - turn letters in different languages into the corresponding ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )"
      ],
      "metadata": {
        "id": "zXGulL2006sV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unicodeToAscii(\"ciàò!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eHdv7nmXsRBH",
        "outputId": "21275cf8-3c3f-4ef4-8373-18b587371127"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ciao!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(s):\n",
        "    # string.lower() to go to lowercase\n",
        "    # string.strip() to eliminate the first (and last) character, if  blank space\n",
        "    # unicodeToAscii(string) to transform to plain ASCII\n",
        "    # re.sub(pattern, repl, string) returns the string obtained by replacing the\n",
        "    #  leftmost non-overlapping occurrences of the pattern in string by the replacement repl\n",
        "    #  - within the pattern, the outermost () indicate that we may want to \"capture\" the pattern and re-use it in the replacement.\n",
        "    #     Indeed, in the replacement we use r\" \\1\" to say that we want to replace the pattern with \" {same_pattern}\". We need to specify\n",
        "    #     r otherwise he considers \"\\\" as a normal backslash character, when instead we want to use it as a special character into \\1\n",
        "    #  - the [] are used because out pattern is composed by several characters, not just one. Indeed, we want to replace \".\", \"!\" and \"?\".\n",
        "    #  - So, the second command will transform \".\" to \" .\", \"!\" to \" !\", \"?\" to \" ?\", \"??\" to \" ? ?\"\n",
        "    #  - In the second command, we do something else.\n",
        "    #    - Again we use [] to indicate a group of characters for the pattern.\n",
        "    #    - We use ^ to indicate that we want to consider as pattern everything that is *not* indicated in the [].\n",
        "    #      a-zA-Z indicate that we don't want to consider lowercase nor uppercase letters (even though we already did lower everything)\n",
        "    #      .!? indicate that we don't want to consider to three characters ., ! and ?\n",
        "    #      So, we want to consider everything that is not a letter, nor a ., nor a !, nor a ?,\n",
        "    #      and we want to delete it and replace it with a single space!\n",
        "    #      But, in this way, a string like \"***\" would be transformed to \"   \", which is not good as later we will use \"   \".split(\" \").\n",
        "    #      To solve this, there is the last + in the pattern string, which says that the pattern can be composed by one or more\n",
        "    #      consecutive characters satisfying the same condition.\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "bA0zSWaH2U3S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Ciao!\".lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4a2Xa1Nr3MvO",
        "outputId": "5c0980f3-d6a6-461f-c227-f09050d9e1af"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ciao!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\" ciao ! \".strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_oD9JbcS2i3s",
        "outputId": "c48d0afc-701c-4fc4-9b4c-5b9708cc0b10"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ciao !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(r\"([?])\", r\" \\1\", \"Ciao??\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s1JAJ5EI3SIz",
        "outputId": "08c07583-af2e-4968-f829-d0c381b53d5c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ciao ? ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(r\"[^a-zA-Z.!?]+\", r\" \", \"a ?*^*:-_=+\"), re.sub(r\"[^a-zA-Z.!?]\", r\" \", \"a ?*^*:-_=+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLzc-Lt466Mp",
        "outputId": "db199aeb-3c7a-4cf1-e1a8-670c15dfac90"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a ? ', 'a ?        ')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the **data**"
      ],
      "metadata": {
        "id": "ocaHgLfH9nES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9w28iUc9mu7",
        "outputId": "5b42da60-2c7a-4cb5-d88a-91c05801ee4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-26 10:18:05--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.35.35.55, 13.35.35.91, 13.35.35.99, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.35.35.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  1.99MB/s    in 1.4s    \n",
            "\n",
            "2024-08-26 10:18:07 (1.99 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"data/eng-fra.txt\"\n",
        "lines = open(filename, encoding = \"utf-8\").read().strip().split(\"\\n\")\n",
        "pairs = [\n",
        "    [\n",
        "        normalizeString(s) for s in l.split(\"\\t\")\n",
        "    ] for l in lines\n",
        "]"
      ],
      "metadata": {
        "id": "xYlx7KR88hnT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBmIpXX7Dstl",
        "outputId": "a08082b6-268c-4366-a827-fb7b9effe1aa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135842"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe5gAq_v_KjC",
        "outputId": "cc506f57-73b9-4aed-bad0-ba0681a6a793"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go .', 'va !']"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(reversed(pairs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiYG3CMLB9lZ",
        "outputId": "bcb9dc53-5c41-4a5b-f83a-057d545f7d33"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go .', 'va !']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_pairs(pairs):\n",
        "    return [list(reversed(pair)) for pair in pairs]"
      ],
      "metadata": {
        "id": "MEENz88-AzVZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the languages for the *translator*"
      ],
      "metadata": {
        "id": "AK7FfgxXC7xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# Command for an English to French translator #############\n",
        "lang1 = \"eng\"\n",
        "lang2 = \"fra\"\n",
        "english_index = 0\n",
        "if pairs[0][0] != \"go .\":\n",
        "  pairs = reverse_pairs(pairs)\n",
        "input_lang, output_lang = Lang(lang1), Lang(lang2)"
      ],
      "metadata": {
        "id": "lD4QpR8i-fDD"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Command for an English to French translator #############\n",
        "lang1 = \"fra\"\n",
        "lang2 = \"eng\"\n",
        "english_index = 1\n",
        "if pairs[0][0] == \"go .\":\n",
        "  pairs = reverse_pairs(pairs)\n",
        "input_lang, output_lang = Lang(lang1), Lang(lang2)"
      ],
      "metadata": {
        "id": "Yxqy5ZGM9N1U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cut in the **sentences**"
      ],
      "metadata": {
        "id": "SjuRxFd4Rqpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we want to introduce an **enormous** cut in the sentences.\n",
        "Among all the 135k sentences, we decide to retain only those that satisfy these requirements.\n",
        "- Both lengths of the sentences in the two languages have to be smaller than MAX_LENGTH = 10\n",
        "- The english sentences have to begin with personal pronoun + present simple of to be"
      ],
      "metadata": {
        "id": "bCH81SnZCm-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is \", \"he s \",   # note that we need to include the blank space, otherwise also the sentence \"he stopped\" would be included\n",
        "    \"she is \", \"she s \",\n",
        "    \"you are \", \"you re \",\n",
        "    \"we are \", \"we re \",\n",
        "    \"they are \", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p, english_index):\n",
        "    condition = True\n",
        "    # Cut on the length of the sentences in the two languages\n",
        "    for i in range(2):\n",
        "      length = len(p[i].split(\" \"))\n",
        "      condition *= (length < MAX_LENGTH)\n",
        "    # Cut that the english sentence has to start with one of the sentences in the eng_prefixes\n",
        "    condition *= (p[english_index].startswith(eng_prefixes))\n",
        "    return condition\n",
        "\n",
        "def filterPairs(pairs, english_index):\n",
        "    return [pair for pair in pairs if filterPair(pair, english_index)]\n",
        "\n",
        "cut_pairs = filterPairs(pairs, english_index)"
      ],
      "metadata": {
        "id": "goWkvyTp739k"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"From {len(pairs)} sentences, we ended up with {len(cut_pairs)} sentences after the cut.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL_3ZRf675g1",
        "outputId": "5f487817-28e4-4296-d4e5-372ad010ea3a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From 135842 sentences, we ended up with 10522 sentences after the cut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rG-8Dg6gE9Oq",
        "outputId": "1377cd78-f10a-4d94-bc75-fe1613df0c31"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'va !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: tokenization of the post-cut sentences using the Lang objects"
      ],
      "metadata": {
        "id": "SiR_ZfFGR2ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Fill the Lang instances\n",
        "if input_lang.n_words == 2:  # if not already filled\n",
        "    for pair in cut_pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])"
      ],
      "metadata": {
        "id": "QZe4X491FJxr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang.name, input_lang.n_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOajG2_OFQ4e",
        "outputId": "e331d48b-43cb-4884-a244-4cc52bd52234"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fra', 4341)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_lang.name, output_lang.n_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3s2nTbPKywJ",
        "outputId": "42b68428-254e-4526-97d2-a757a39de6ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('eng', 2802)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: transform the tokenization results into torch.tensor objects\n",
        "\n"
      ],
      "metadata": {
        "id": "5d7iC_LMSCFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to transform the cut_pairs into something pytorch-friendly using the tokenization provided by the attributes inside the two Lang objects that we just filled."
      ],
      "metadata": {
        "id": "og6DjZmlMuZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorFromSentence(lang, sentence):\n",
        "    \"\"\"\n",
        "    This function\n",
        "    - separates the sentence into words,\n",
        "    - transforms the vector of words into a vector of indices,\n",
        "    - appends the EOS token, i.e. 1\n",
        "    - transforms the indices vector into a torch.tensor with shape (len(indices), 1) using the torch.Tensor.view object.\n",
        "    \"\"\"\n",
        "    words = sentence.split(\" \")\n",
        "    indices = [lang.word2index[word] for word in words]\n",
        "    indices.append(EOS_token)\n",
        "    return torch.tensor(indices, dtype = torch.long, device = device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    \"\"\"\n",
        "    This function takes a single pair vector, which contains the sentences in the two languages,\n",
        "    and pass each of the sentences, with the corresponding Lang objects, to the tensorFromSentence method.\n",
        "    This function returns the tuple of the input and output tensors.\n",
        "    \"\"\"\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "XbVMQDiCK_LA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ptt = tensorFromSentence(output_lang, \"i will try to fix you\")\n",
        "print (ptt)\n",
        "print (ptt.shape), print (ptt.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoitmlSEOuJ_",
        "outputId": "14ead385-c3f2-4aa1-bb54-8f73a89b1960"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2],\n",
            "        [1700],\n",
            "        [1096],\n",
            "        [ 532],\n",
            "        [2218],\n",
            "        [ 129],\n",
            "        [   1]])\n",
            "torch.Size([7, 1])\n",
            "torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an RNN encoder"
      ],
      "metadata": {
        "id": "RJOwW1V6Sdx-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V3Js80wMRR5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an RNN decoder"
      ],
      "metadata": {
        "id": "o2y8m3IISlo8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MiozxasSqhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train using a single sentence"
      ],
      "metadata": {
        "id": "1BSfXQavSv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p05AvIesS0E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training iteration over the whole training set"
      ],
      "metadata": {
        "id": "7vMW2YN4S0gW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1zjTSGFS7dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance evaluation on the test set"
      ],
      "metadata": {
        "id": "J7_aaOH7S73x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nh9oCwt1TBKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}