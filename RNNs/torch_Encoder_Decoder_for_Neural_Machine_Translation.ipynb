{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc2kXzs7EImuMMl8O2WZHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cerasole/ml4hep/blob/main/RNNs/torch_Encoder_Decoder_for_Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine translation using RNNs\n",
        "\n",
        "[It is suggested to run this notebook using GPUs, i.e. in Colab]\n",
        "\n",
        "The theory of machine translation using RNNs is described, for instance, in Chapter 16 of https://github.com/ageron/handson-ml3.\n",
        "\n",
        "Basics of the algorithm\n",
        "- Input: sentence in a given language,\n",
        "- Output: translation of the input sentence in a different language.\n",
        "\n",
        "The architecture consists in an encoder-decoder system.\n",
        "- The input sentence is fed to the encoder, which transforms the input into a low-dimensional latent representation\n",
        "- The decoder transforms the latent representation into an output sentence."
      ],
      "metadata": {
        "id": "H8aW01b9m1_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4PVrt2XsjCI-"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ts-ggQo_4G",
        "outputId": "c1816428-e76a-4986-dc56-9af7b5455529"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language class for tokenization"
      ],
      "metadata": {
        "id": "6u8C4oJaRWk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a class for handling languages and sentences.\n",
        "\n",
        "This class will need to\n",
        "- register the words in the sentences given as input to the class\n",
        "\n",
        "In other words, we employ this class to perform the tokenization = encode every word as an integer.\n",
        "\n",
        "In the preprocessing, we will uniform the sentences to a standard. This is is not included in the Lang class and it is a fundamental preprocessing step."
      ],
      "metadata": {
        "id": "DhJySh2Lr10i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0   # Start-of-Sentence\n",
        "EOS_token = 1   # End-of-Sentence\n",
        "\n",
        "class Lang:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name # name of the language\n",
        "        self.word2index = {} # dictionary containing words: indices when they were first inserted in the dictionary\n",
        "        self.word2count = {} # dictionary containing words: number of times they were inserted in the dictionary\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # dictionary contiaining indices of first insertion in the dictionary: words\n",
        "        self.n_words = 2  # Total counts of words in the dictionary, including SOS and EOS\n",
        "\n",
        "    def addWord(self, word):\n",
        "        # When we add a word, we have to check if it is already present in the dictionary, e.g. in the word2index.\n",
        "        # If it is not present, we need to\n",
        "        #  - add this word to the self.word2index dictionary, giving it as index the current self.n_words,\n",
        "        #  - add this word to the self.word2count dictionary, giving it 1 count,\n",
        "        #  - add, in the self.index2word dictionary, using as index given by self.n_words, the word itself,\n",
        "        #  - increase by 1 the number of total words, aka self.n_words.\n",
        "        # If it is already present, we need to\n",
        "        #  - increase by 1 the corresponding self.word2counts entry.\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        # Add words splitting the sentence by space.\n",
        "        # It would be better if the sentence is already standardized.\n",
        "        # We will take care of this in the preprocessing of the sentences.\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n"
      ],
      "metadata": {
        "id": "4Lq1ePc9pFpF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang = Lang(\"prova\")\n",
        "print (lang.word2count, lang.word2index, lang.index2word, lang.n_words)\n",
        "lang.addSentence(\"could you please stop the noise?\")\n",
        "print (lang.word2count, lang.word2index, lang.index2word, lang.n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lV19B5uzrHzu",
        "outputId": "fb415c27-52ff-4ede-df2b-148342938535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{} {} {0: 'SOS', 1: 'EOS'} 2\n",
            "{'could': 1, 'you': 1, 'please': 1, 'stop': 1, 'the': 1, 'noise?': 1} {'could': 2, 'you': 3, 'please': 4, 'stop': 5, 'the': 6, 'noise?': 7} {0: 'SOS', 1: 'EOS', 2: 'could', 3: 'you', 4: 'please', 5: 'stop', 6: 'the', 7: 'noise?'} 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: standardization of the sentences"
      ],
      "metadata": {
        "id": "3iGBuGqoRbd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cells, we will investigate methods to standardize the input sentences:\n",
        "- transform to lower case\n",
        "- all special characters need to be treated differently\n",
        "- Take into account abbreviations"
      ],
      "metadata": {
        "id": "TzOUkimX07an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/a/518232/2809427\n",
        "# Turn a Unicode string to plain ASCIIi, i.e.\n",
        "# - remove the accents without changing the letter\n",
        "# - turn letters in different languages into the corresponding ASCII\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )"
      ],
      "metadata": {
        "id": "zXGulL2006sV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unicodeToAscii(\"ciàò!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eHdv7nmXsRBH",
        "outputId": "21275cf8-3c3f-4ef4-8373-18b587371127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ciao!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeString(s):\n",
        "    # string.lower() to go to lowercase\n",
        "    # string.strip() to eliminate the first (and last) character, if  blank space\n",
        "    # unicodeToAscii(string) to transform to plain ASCII\n",
        "    # re.sub(pattern, repl, string) returns the string obtained by replacing the\n",
        "    #  leftmost non-overlapping occurrences of the pattern in string by the replacement repl\n",
        "    #  - within the pattern, the outermost () indicate that we may want to \"capture\" the pattern and re-use it in the replacement.\n",
        "    #     Indeed, in the replacement we use r\" \\1\" to say that we want to replace the pattern with \" {same_pattern}\". We need to specify\n",
        "    #     r otherwise he considers \"\\\" as a normal backslash character, when instead we want to use it as a special character into \\1\n",
        "    #  - the [] are used because out pattern is composed by several characters, not just one. Indeed, we want to replace \".\", \"!\" and \"?\".\n",
        "    #  - So, the second command will transform \".\" to \" .\", \"!\" to \" !\", \"?\" to \" ?\", \"??\" to \" ? ?\"\n",
        "    #  - In the second command, we do something else.\n",
        "    #    - Again we use [] to indicate a group of characters for the pattern.\n",
        "    #    - We use ^ to indicate that we want to consider as pattern everything that is *not* indicated in the [].\n",
        "    #      a-zA-Z indicate that we don't want to consider lowercase nor uppercase letters (even though we already did lower everything)\n",
        "    #      .!? indicate that we don't want to consider to three characters ., ! and ?\n",
        "    #      So, we want to consider everything that is not a letter, nor a ., nor a !, nor a ?,\n",
        "    #      and we want to delete it and replace it with a single space!\n",
        "    #      But, in this way, a string like \"***\" would be transformed to \"   \", which is not good as later we will use \"   \".split(\" \").\n",
        "    #      To solve this, there is the last + in the pattern string, which says that the pattern can be composed by one or more\n",
        "    #      consecutive characters satisfying the same condition.\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "bA0zSWaH2U3S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"Ciao!\".lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4a2Xa1Nr3MvO",
        "outputId": "5c0980f3-d6a6-461f-c227-f09050d9e1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ciao!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\" ciao ! \".strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_oD9JbcS2i3s",
        "outputId": "c48d0afc-701c-4fc4-9b4c-5b9708cc0b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ciao !'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(r\"([?])\", r\" \\1\", \"Ciao??\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s1JAJ5EI3SIz",
        "outputId": "08c07583-af2e-4968-f829-d0c381b53d5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ciao ? ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re.sub(r\"[^a-zA-Z.!?]+\", r\" \", \"a ?*^*:-_=+\"), re.sub(r\"[^a-zA-Z.!?]\", r\" \", \"a ?*^*:-_=+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLzc-Lt466Mp",
        "outputId": "db199aeb-3c7a-4cf1-e1a8-670c15dfac90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a ? ', 'a ?        ')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the **data**"
      ],
      "metadata": {
        "id": "ocaHgLfH9nES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9w28iUc9mu7",
        "outputId": "09ae91a2-abc7-4190-f39e-1326f06dce4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-27 03:49:31--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.160.10.36, 18.160.10.22, 18.160.10.28, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.160.10.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-08-27 03:49:31 (25.9 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"data/eng-fra.txt\"\n",
        "lines = open(filename, encoding = \"utf-8\").read().strip().split(\"\\n\")\n",
        "pairs = [\n",
        "    [\n",
        "        normalizeString(s) for s in l.split(\"\\t\")\n",
        "    ] for l in lines\n",
        "]"
      ],
      "metadata": {
        "id": "xYlx7KR88hnT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(pairs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBmIpXX7Dstl",
        "outputId": "895fba8b-0fc0-466b-f9f7-4f1114f73b95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "135842"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe5gAq_v_KjC",
        "outputId": "84077ff4-548f-4558-e048-3833a362c44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go .', 'va !']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(reversed(pairs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiYG3CMLB9lZ",
        "outputId": "88fc6722-fbdd-46b9-fb04-53c2dea9b93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['va !', 'go .']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_pairs(pairs):\n",
        "    return [list(reversed(pair)) for pair in pairs]"
      ],
      "metadata": {
        "id": "MEENz88-AzVZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set the languages for the *translator*"
      ],
      "metadata": {
        "id": "AK7FfgxXC7xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############# Command for an English to French translator #############\n",
        "lang1 = \"eng\"\n",
        "lang2 = \"fra\"\n",
        "english_index = 0\n",
        "if pairs[0][0] != \"go .\":\n",
        "  pairs = reverse_pairs(pairs)\n",
        "input_lang, output_lang = Lang(lang1), Lang(lang2)"
      ],
      "metadata": {
        "id": "lD4QpR8i-fDD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Command for an English to French translator #############\n",
        "lang1 = \"fra\"\n",
        "lang2 = \"eng\"\n",
        "english_index = 1\n",
        "if pairs[0][0] == \"go .\":\n",
        "  pairs = reverse_pairs(pairs)\n",
        "input_lang, output_lang = Lang(lang1), Lang(lang2)"
      ],
      "metadata": {
        "id": "Yxqy5ZGM9N1U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cut in the **sentences**"
      ],
      "metadata": {
        "id": "SjuRxFd4Rqpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we want to introduce an **enormous** cut in the sentences.\n",
        "Among all the 135k sentences, we decide to retain only those that satisfy these requirements.\n",
        "- Both lengths of the sentences in the two languages have to be smaller than MAX_LENGTH = 10\n",
        "- The english sentences have to begin with personal pronoun + present simple of to be"
      ],
      "metadata": {
        "id": "bCH81SnZCm-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is \", \"he s \",   # note that we need to include the blank space, otherwise also the sentence \"he stopped\" would be included\n",
        "    \"she is \", \"she s \",\n",
        "    \"you are \", \"you re \",\n",
        "    \"we are \", \"we re \",\n",
        "    \"they are \", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p, english_index):\n",
        "    condition = True\n",
        "    # Cut on the length of the sentences in the two languages\n",
        "    for i in range(2):\n",
        "      length = len(p[i].split(\" \"))\n",
        "      condition *= (length < MAX_LENGTH)\n",
        "    # Cut that the english sentence has to start with one of the sentences in the eng_prefixes\n",
        "    condition *= (p[english_index].startswith(eng_prefixes))\n",
        "    return condition\n",
        "\n",
        "def filterPairs(pairs, english_index):\n",
        "    return [pair for pair in pairs if filterPair(pair, english_index)]\n",
        "\n",
        "cut_pairs = filterPairs(pairs, english_index)"
      ],
      "metadata": {
        "id": "goWkvyTp739k"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (f\"From {len(pairs)} sentences, we ended up with {len(cut_pairs)} sentences after the cut.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL_3ZRf675g1",
        "outputId": "1b73550c-8715-447c-d10c-dd7145f62808"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From 135842 sentences, we ended up with 10522 sentences after the cut.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[0][0], cut_pairs[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG-8Dg6gE9Oq",
        "outputId": "4e1bbc24-77eb-4081-e94d-abc38cf4ebd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('va !', 'j ai ans .')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: tokenization of the post-cut sentences using the Lang objects"
      ],
      "metadata": {
        "id": "SiR_ZfFGR2ZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Fill the Lang instances\n",
        "if input_lang.n_words == 2:  # if not already filled\n",
        "    for pair in cut_pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])"
      ],
      "metadata": {
        "id": "QZe4X491FJxr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang.name, input_lang.n_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOajG2_OFQ4e",
        "outputId": "c33a0e7d-b755-4521-d336-85fdd86974c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fra', 4341)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_lang.name, output_lang.n_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3s2nTbPKywJ",
        "outputId": "b89d9335-46f1-4f10-bc3c-e9d1f939afb0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('eng', 2802)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing: transform the tokenization results into torch.tensor objects\n",
        "\n"
      ],
      "metadata": {
        "id": "5d7iC_LMSCFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to transform the cut_pairs into something pytorch-friendly using the tokenization provided by the attributes inside the two Lang objects that we just filled."
      ],
      "metadata": {
        "id": "og6DjZmlMuZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorFromSentence(lang, sentence):\n",
        "    \"\"\"\n",
        "    This function\n",
        "    - separates the sentence into words,\n",
        "    - transforms the vector of words into a vector of indices,\n",
        "    - appends the EOS token, i.e. 1\n",
        "    - transforms the indices vector into a torch.tensor with shape (len(indices), 1) using the torch.Tensor.view object.\n",
        "    \"\"\"\n",
        "    words = sentence.split(\" \")\n",
        "    indices = [lang.word2index[word] for word in words]\n",
        "    indices.append(EOS_token)\n",
        "    return torch.tensor(indices, dtype = torch.long, device = device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    \"\"\"\n",
        "    This function takes a single pair vector, which contains the sentences in the two languages,\n",
        "    and pass each of the sentences, with the corresponding Lang objects, to the tensorFromSentence method.\n",
        "    This function returns the tuple of the input and output tensors.\n",
        "    \"\"\"\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "XbVMQDiCK_LA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Av8Y5JOg9ZNV",
        "outputId": "a4bdd716-9699-49ba-e320-6d8be495b1a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eng'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ptt = tensorFromSentence(output_lang, \"i will try to fix you\")\n",
        "print (ptt)\n",
        "print (ptt.shape), print (ptt.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoitmlSEOuJ_",
        "outputId": "eb9bd539-2383-481f-c50b-3755f4539137"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   2],\n",
            "        [1700],\n",
            "        [1096],\n",
            "        [ 532],\n",
            "        [2218],\n",
            "        [ 129],\n",
            "        [   1]])\n",
            "torch.Size([7, 1])\n",
            "torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "### Useful preliminary: embeddings\n",
        "\n",
        "[See pages 430-439 of the A. Geron book, \"Hands-on Machine Learning with Sklearn, Keras and Tensorflow\"]\n",
        "\n",
        "Embeddings are used in the preprocessing step. If data consists in categorical or text features, they need to be converted to numerical features.\n",
        "\n",
        "For instance, consider a categorical feature such as the location \"<=1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\" of the California housing dataset [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html].\n",
        "Or, consider the words of each vocabulary that we're considering in this notebook.\n",
        "\n",
        "It is mandatory to encode these features into a numerical feature before using it in a NN.\n",
        "\n",
        "##### **Dimension = 1**\n",
        "As first choice, it is possible to map each category to its index (0 to 4), possibly including a 5 index for out-of-vocabulary (oov) instances.\n",
        "Such a choice has the drawback that categories with close indices (e.g. 2 and 3) are considered by the NN closer than categories with distant indices (e.g. 1 and 5).\n",
        "Such a distance is a product of the choice of mapping the categories with numerical indices and has nothing to do with the categories themselves. So, this results in a bad choice and introduces bias in the network.\n",
        "\n",
        "[NB: Notice that this is what we did when we tokenized the words of the two languages: we mapped each word to an index.\n",
        "However, if we used these indices directly in the NN, we would introduce that the words with indices 225 and 226 are closer than those with indices 10 and 450, which has no sense, as they are random words in a language, which have those indices only because we fed the Lang objects in a given order.]\n",
        "\n",
        "##### **Dimension = number N of categories**\n",
        "As second choice, we can use one-hot vectors: starting from the indices defined earlier, we can associate the category with index i (out of N) to the vector of length N having all zero values apart from 1 in the i-th position.\n",
        "It's like we are adding N features to the dataset.\n",
        "Equivalently, now each instance would become an array of N values.\n",
        "This can be good when the number of dimensions is low, but it becomes less efficient for wide datasets.\n",
        "\n",
        "##### **So, let's take an intermediate number of dimensions between 1 and N?**\n",
        "If so, let's take a number D of dimensions (indicated as embedding_dim in pytorch), intermediate between 1 and N.\n",
        "This number D is a hyperparameter that can be tweaked, so we can try different values for D and see what changes.\n",
        "- For one-hot vectors, with dimension N, each category became associate to a vector containing zero everywhere except for 1 index.\n",
        "- In embeddings, the situation is different, because embeddings are trainable!\n",
        "At first, for each instance, a vector with dimension D is initialized randomly (equivalently, a matrix of input_size x D is initialized randomly). Then, the embeddings have parameters that are tuned so that each category, after training, will become clustered in some regions of the parameters' space, and hopefully different categories will cluster in different parameters' regions.\n",
        "\n",
        "Trivially, the number of parameters of an embedding is equal to the product of num_embeddings and embedding_dim.\n",
        "\n",
        "This is called 'representation learning'.\n",
        "\n",
        "================================================================\n",
        "\n",
        "In the following example, we create an embedding with 60 input dimensions (input_size = 60) and 10 hidden dimensions (hidden_size = 10).\n",
        "\n",
        "So, essentially, it is a 60 x 10 matrix, which has a row per input dimension and a column for hidden dimension.\n",
        "\n",
        "The tensor t has, inside, the categories number 3, 4 and 58. It is necessary that the input size (= first argument of the Embedding) is larger than 58+1, then, of course. So, input_size >= 59. Otherwise, it will give an error later."
      ],
      "metadata": {
        "id": "_v39CfO-r-Zj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "emb = nn.Embedding(60, 10, device = device)"
      ],
      "metadata": {
        "id": "2QlCrYD39K3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([3, 4, 58], dtype = torch.long, device = device)\n",
        "emb(t)\n",
        "# This is the representation of the 3 categories with indices 3, 4 and 58."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fEPKU8i-4vZ",
        "outputId": "b0d593f3-a3b8-450d-b436-70c7c8072441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.3775, -1.1485,  1.1086,  0.4524,  1.1358,  0.6782,  1.2561, -0.3769,\n",
              "          1.4632, -2.3228],\n",
              "        [-1.7808, -1.4599,  1.8347, -0.1513, -1.4200,  0.5593, -0.8887, -0.2016,\n",
              "          1.3862,  0.5823],\n",
              "        [-0.4966, -0.0085,  1.2775, -0.4772,  1.3190, -1.2386, -1.5081,  1.1757,\n",
              "         -0.3952, -0.6479]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([3, 4, 58], dtype = torch.long, device = device).view(-1, 1)\n",
        "emb(t)#.view(-1, 1, 10)  <--  this view is useless!\n",
        "# Same as before, just a different shape because of the .view method\n",
        "# --> we're considering separate categories, each instance has only 1 category.\n",
        "\n",
        "# NB. Pytorch RNNs takes arrays with shape (-1, 1, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr-NwOLc-_1T",
        "outputId": "edb86c2f-e72a-40b9-e582-694153760133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.3775, -1.1485,  1.1086,  0.4524,  1.1358,  0.6782,  1.2561,\n",
              "          -0.3769,  1.4632, -2.3228]],\n",
              "\n",
              "        [[-1.7808, -1.4599,  1.8347, -0.1513, -1.4200,  0.5593, -0.8887,\n",
              "          -0.2016,  1.3862,  0.5823]],\n",
              "\n",
              "        [[-0.4966, -0.0085,  1.2775, -0.4772,  1.3190, -1.2386, -1.5081,\n",
              "           1.1757, -0.3952, -0.6479]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in our case, the input size of the encoder is for sure equal to input_lang.n_words.\n",
        "\n",
        "Differently, the hidden size is a hyper parameter that we can tweak."
      ],
      "metadata": {
        "id": "edidrBxN_2Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an RNN encoder\n",
        "\n",
        "As standard in Pytorch, a model is a subclass of the torch.nn.Module class and needs to have a forward method for evaluating the model for a given instance.\n",
        "\n",
        "**constructor**\n",
        "\n",
        "In this case, the encoder has 2 argument inputs:\n",
        "- input_size, which is NOT the number of words of the input sentence, but the number of words of the input_lang object ($\\approx$ 4.3k for French).\n",
        "- hidden_size, which is the number of dimensions of the embedding to be used.\n",
        "\n",
        "1 - As usual, the first thing to do is to use the constructor of the parent class. This is done as `super().__init__()`, possibly passing some arguments that were given to the constructor.\n",
        "\n",
        "2 - Then, we save as attributes the input and hidden sizes.\n",
        "\n",
        "3 - Then, we define the attributes of the layers that we're including in the encoder: an embedding for treating the categorical inputs and a GRU (simply, an optimized recurrent layer).\n",
        "\n",
        "**forward**\n",
        "\n",
        "1 - We need to feed the self.embedding with the input tensor (which represents a sequence of word indices) and as output we want a vector of size (-1, 1, hidden_size) because the GRU takes it like that\n",
        "\n",
        "2 - We need to pass to the GRU both the embedding output and hidden, which is (related to) exactly the previous state of the GRU (remind that it's a Recurrent Neural Network, so the output at step j depends on the output at step j-1)\n",
        "\n",
        "**init_hidden**\n",
        "\n",
        "1 - For the first step, the output of the previous layer does not exist, so we just initialize it to 0. Of course, it needs to have the same shape as the input of the GRU, which we shaped as (-1, 1, self.hidden_size)\n"
      ],
      "metadata": {
        "id": "RJOwW1V6Sdx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN (torch.nn.Module):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "      #\n",
        "      super().__init__()\n",
        "      #\n",
        "      self.input_size = input_size\n",
        "      self.hidden_size = hidden_size\n",
        "      #\n",
        "      self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "      self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "      return\n",
        "\n",
        "    def forward (self, input, hidden):\n",
        "      embedded = self.embedding(input).view(-1, 1, self.hidden_size)  # this view might be useless if we already shaped input as (-1, 1)\n",
        "      output, hidden = self.gru(embedded, hidden)\n",
        "      return output, hidden\n",
        "\n",
        "    def initHidden (self):\n",
        "      return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "metadata": {
        "id": "V3Js80wMRR5r"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "encoder = EncoderRNN (input_size = input_lang.n_words, hidden_size = hidden_size)"
      ],
      "metadata": {
        "id": "qwv8st95U8yu"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    # Conta dei parametri\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Numero totale di parametri: {total_params}\")\n",
        "\n",
        "    # Visualizzazione dei parametri specifici\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"Nome: {name}, Shape: {param.shape}, Numero di parametri: {param.numel()}\")\n",
        "\n",
        "count_parameters(encoder)  ### i = input, h = hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xz5as8bWVaQ",
        "outputId": "45c8265f-c5bf-4657-932a-03af0ea6af41"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero totale di parametri: 1506048\n",
            "Nome: embedding.weight, Shape: torch.Size([4341, 256]), Numero di parametri: 1111296\n",
            "Nome: gru.weight_ih_l0, Shape: torch.Size([768, 256]), Numero di parametri: 196608\n",
            "Nome: gru.weight_hh_l0, Shape: torch.Size([768, 256]), Numero di parametri: 196608\n",
            "Nome: gru.bias_ih_l0, Shape: torch.Size([768]), Numero di parametri: 768\n",
            "Nome: gru.bias_hh_l0, Shape: torch.Size([768]), Numero di parametri: 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define an RNN decoder\n",
        "\n",
        "The RNN decoder has to process\n",
        "- the output of the encoders\n",
        "- the sentences of the translations\n",
        "\n",
        "So, we need to give as argument to the class\n",
        "- the dimension of the output of the encoder, i.e. hidden_size\n",
        "- analogously to the input_size for the encoder, the output_size, which will be the number of words of the target language (output_lang.n_words, which was $\\approx$ 2.8k).\n",
        "\n",
        "**constructor**\n",
        "- After the same trivial things as before, we have to define an embedding from the output_size to the hidden_size\n",
        "- We also have to define a GRU that goes from hidden_size dimension to hidden_size dimension\n",
        "- We define a nn.Linear from hidden_size to output_size and an activation layer of LogSoftMax with dim = 1\n",
        "\n",
        "**forward**\n",
        "- The embedding takes the input and shapes it as desired\n",
        "- **NB: DIFFERENTLY FROM BEFORE, THE EMBEDDED OUTPUT IS PASSED TO A RELU LAYER BEFORE PASSING TO THE GRU**. This does not change the dimension and shapes\n",
        "- Then, we pass it to the GRU, giving also the hidden as usual\n",
        "- **NB: DIFFERENTLY THAN BEFORE, WE NOW GO THROUGH A LINEAR LAYER WITH LOGSOFTMAX ACTIVATION FUNCTION**. Also this step does not change the shape, which is still (-1, 1, self.hidden_size)."
      ],
      "metadata": {
        "id": "o2y8m3IISlo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN (torch.nn.Module):\n",
        "\n",
        "    def __init__ (self, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        #\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        #\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim = 1)\n",
        "        return\n",
        "\n",
        "    def forward (self, input, hidden):\n",
        "        embedded = self.embedding(input).view(-1, 1, self.hidden_size)\n",
        "        output = F.relu(embedded)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden (self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "metadata": {
        "id": "8MiozxasSqhx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = nn.LogSoftmax(dim=1)\n",
        "input = torch.randn(2, 3)\n",
        "output = m(input)\n",
        "input.shape, output.shape  # This is just to show that LogSoftMax does not change the shape of the tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YFGvyqzSr-e",
        "outputId": "5d6b6512-7a10-42da-9c0d-14902f9aa74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3]), torch.Size([2, 3]))"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 256\n",
        "decoder = DecoderRNN (hidden_size = hidden_size, output_size = output_lang.n_words)"
      ],
      "metadata": {
        "id": "-tLw-THtVLPO"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters (decoder)  ### i = input, h = hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TPOnJGZXBpR",
        "outputId": "0340f4d5-1205-4633-956b-aaed2c2005a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero totale di parametri: 1832178\n",
            "Nome: embedding.weight, Shape: torch.Size([2802, 256]), Numero di parametri: 717312\n",
            "Nome: gru.weight_ih_l0, Shape: torch.Size([768, 256]), Numero di parametri: 196608\n",
            "Nome: gru.weight_hh_l0, Shape: torch.Size([768, 256]), Numero di parametri: 196608\n",
            "Nome: gru.bias_ih_l0, Shape: torch.Size([768]), Numero di parametri: 768\n",
            "Nome: gru.bias_hh_l0, Shape: torch.Size([768]), Numero di parametri: 768\n",
            "Nome: out.weight, Shape: torch.Size([2802, 256]), Numero di parametri: 717312\n",
            "Nome: out.bias, Shape: torch.Size([2802]), Numero di parametri: 2802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training over the whole training dataset\n",
        "\n",
        "How can we train this?\n",
        "\n",
        "0 - Define a sequence of sentences the algorithm will be trained on. But for now we will train only using few sentences, just to see how it works\n",
        "\n",
        "1 - We need to define some hyper parameters, such as the learning rate\n",
        "\n",
        "2 - We need to define the optimizers of the encoder and decoder\n",
        "\n",
        "3 - We need to specify the criterion to compute the loss.\n",
        "We use the Negative Log Likelihood Loss (nn.NLLLoss) because this is a classification problem with lots of classes [https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html].\n",
        "\n",
        "\\begin{equation}\n",
        "    \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
        "    l_n = - w_{y_n} x_{n,y_n}, \\quad\n",
        "    w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},\n",
        "\\end{equation}\n",
        "where `x` is the input, `y` is the target, `w` is the weight, and `N` is the batch size.\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\ell(x, y) = \\begin{cases}\n",
        "        \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &\n",
        "        \\text{if reduction} = \\text{`mean';}\\\\\n",
        "        \\sum_{n=1}^N l_n,  &\n",
        "        \\text{if reduction} = \\text{`sum'.}\n",
        "    \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "4 - Do a loop for each training instance, compute the loss for each training instance, and possibly sum the various losses to have the cumulative one.\n",
        "[Remind that input tensor is the tensor of the indices of a sentence, shaped as (-1, 1, len(sentence tokens)).]\n",
        "\n",
        "5 -  We may like to print the average loss and elapsed time every `print_every` iterations."
      ],
      "metadata": {
        "id": "1BSfXQavSv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### In the original notebook, this is the beginning of the trainIters method\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "#0\n",
        "niters = 10\n",
        "training_pairs = [tensorsFromPair(random.choice(cut_pairs)) for i in range(niters)]\n",
        "print_every = 1\n",
        "loss_partial = 0\n",
        "\n",
        "#1\n",
        "learning_rate = 0.01\n",
        "\n",
        "#2\n",
        "encoder_optimizer = optim.SGD(params = encoder.parameters(), lr = learning_rate, nesterov = False)\n",
        "decoder_optimizer = optim.SGD(params = decoder.parameters(), lr = learning_rate, nesterov = False)\n",
        "\n",
        "#3\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "#4\n",
        "for iter in range(1, niters + 1):\n",
        "    tensor_pair = training_pairs[iter-1]\n",
        "    input_tensor, output_tensor = tensor_pair[0], tensor_pair[1]\n",
        "    # Remind that\n",
        "    loss = train (input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    loss_total += loss\n",
        "\n",
        "    #5\n",
        "    if iter % print_every == 0:\n",
        "        avg_loss = loss_partial / print_every\n",
        "        loss_partial = 0\n",
        "        seconds = time.time() - start\n",
        "        minutes = int(seconds / 60)\n",
        "        hour_string = \"%d min, %.1f sec\" % (minutes, seconds)\n",
        "        print (f\"{iter} / {niters}, \", hour_string, f\" - Average loss in the last {print_every} iters = {avg_loss:.3f}\")"
      ],
      "metadata": {
        "id": "p05AvIesS0E7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "e6194ee4-f8ff-436b-ca36-ca0e4375c61c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-cfce10a37570>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Remind that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training using a single pair\n",
        "\n",
        "For a single training instance, this is what we have to do.\n",
        "\n",
        "0 - We're using RNN-based encoder and decoder: we have to give each sentence by giving word by word in succession. Each word is an index.\n",
        "- The first word is the first step.\n",
        "- The second word is the second step and so on.\n",
        "\n",
        "As it is an RNN, at step j we need to give as input also the output of step j-1.\n",
        "At step 0, the output of the previous step of the GRU has to be initialized to 0. This is done through the encoder_hidden variable.\n",
        "\n",
        "1 - We need to set the gradients of the optimizers of the encoder and decoder (we're using SGD for both) to zero because we will have to perform the optimization using the sequences of tokens inside the input and output tensors.\n",
        "\n",
        "2 - In the first place, we will need to loop over the tokens of the input tensor, so it's good to compute the lengths of the two (input and output) tensors.\n",
        "\n",
        "3 - We do a loop over the tokens of the input tensor. For each token, we have a tensor with shape (1,1) that contains the index of the token. We feed the encoder with this input (1,1) tensor and hidden layer with size (1, 1, hidden_size).\n",
        "\n",
        "What will the encoder do?\n",
        "\n",
        "It will compute the embedding of the input token, producing a tensor with shape (-1, 1, hidden_size). Then, the GRU will receive as input this embedded tensor and the hidden tensor (the output of the previous step, otherwise 0 tensor for the first step) and compute encoder_output and encoder_hidden. These are the SAME tensors, by definition of RNN (= the output is given as input to the next step).\n",
        "\n",
        "The last input token is torch.tensor([1]), which corresponds to the EOS token.\n",
        "After the loop on the input tensor, we finally have the encoder_output and encoder_hidden tensors (which, I recall to you, are always the same, because the output of an RNN at step j is given as additional input to the RNN at step j+1)\n",
        "\n",
        "4 - We will have to loop over the size of the output tensor.\n",
        "We define the initial input and hidden state of the decoder as follows:\n",
        " - the initial input of the decoder is a tensor with a single token, that is the [[0]], the SOS token.\n",
        " - the hidden state of the decoder is, instead, THE OUTPUT OF THE ENCODER (or, equivalently, the hidden state of the encoder)\n",
        "\n",
        "5 - Now, we finally make the loop over the target tensor.\n",
        " - At the first step, decoder_input is the torch.tensor([[SOS_token]], device = device), while the decoder_hidden is the encoder output.\n",
        " - In the next steps,\n",
        "   - the decoder_input csn be the prediction of the decoder itself! (no teaching forcing)\n",
        "   Otherwise, it can be the actual target tensor token (teacher forcing)\n",
        "   - the decoder_hidden is simply the output of the previous step of the GRU within the decoder\n",
        "\n",
        "   Notice that, in this case, the output of the decoder is something with dimension given by output_lang.n_words, whereas the hidden state of the GRU within the decoder has shape given by hidden_size, so the two tensors are not the same in this case.\n"
      ],
      "metadata": {
        "id": "7vMW2YN4S0gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Now we have defined everything that is passed to the train method\n",
        "input_tensor, output_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion;"
      ],
      "metadata": {
        "id": "h1zjTSGFS7dW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTes8NhMIdFB",
        "outputId": "3b92760b-f2ab-4e12-8bcd-d030510c57a5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#0\n",
        "encoder_hidden = encoder.initHidden()   # tensor of zeros of shape (1, 1, encoder.hidden_size)\n",
        "\n",
        "#1\n",
        "encoder_optimizer.zero_grad()\n",
        "decoder_optimizer.zero_grad()\n",
        "\n",
        "#2\n",
        "length_input_tensor, length_output_tensor = input_tensor.size(0), output_tensor.size(0)\n",
        "print (f\"Input sentence has {length_input_tensor:d} tokens, whereas output sentence has {length_output_tensor:d} tokens.\\n\")\n",
        "\n",
        "def sentenceFromTensor (tensor, lang):\n",
        "    s = \"\"\n",
        "    length_tensor = tensor.size(0)\n",
        "    for i in range(length_tensor):\n",
        "        s += lang.index2word[int(tensor[i][0])] + \" \"\n",
        "    s = s.strip()\n",
        "    return s\n",
        "\n",
        "print (f\"Input sentence: '{sentenceFromTensor(input_tensor, input_lang)}'\")\n",
        "print (f\"Output sentence: '{sentenceFromTensor(output_tensor, output_lang)}'\")\n",
        "\n",
        "#3\n",
        "loss = 0\n",
        "for ei in range(length_input_tensor):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)  ### at each step, the two tensors encoder_output and encoder_hidden are the same!\n",
        "\n",
        "#4\n",
        "# Initial input of the decoder\n",
        "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "# Initial hidden state of the decoder\n",
        "decoder_hidden = encoder_output # or, equivalently, encoder_hidden, as they are identical tensors.\n",
        "\n",
        "#5\n",
        "teaching_forcing_ratio = 0.2\n",
        "use_teacher_forcing = random.random() > teaching_forcing_ratio\n",
        "\n",
        "for di in range(length_output_tensor):\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hQqW6Z8ImkR",
        "outputId": "0ae05849-124d-42c6-ae28-88a8e5e36d31"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentence has 6 tokens, whereas output sentence has 6 tokens.\n",
            "\n",
            "Input sentence: 'c est mon associe . EOS'\n",
            "Output sentence: 'he s my partner . EOS'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# these are some interesting tests made during the training step"
      ],
      "metadata": {
        "id": "t0OwfQ-wSujk"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor[0], encoder_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWlcVNjENsKA",
        "outputId": "ef2faf31-0cf8-4f25-8125-eb95b951aa1b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([145]),\n",
              " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "           0., 0., 0.]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_first_output, encoder_first_hidden = encoder(input_tensor[0], encoder_hidden)"
      ],
      "metadata": {
        "id": "7S7lPCreLiSH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_first_output == encoder_first_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTUFZWtPN6ox",
        "outputId": "35fbaf96-4591-4f48-8d0e-abe91667a573"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True]]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_second_output, encoder_second_hidden = encoder(input_tensor[1], encoder_first_hidden)"
      ],
      "metadata": {
        "id": "a9dVy-G1N-LL"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_second_output == encoder_second_hidden"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTaQOXlHPwL9",
        "outputId": "8ae6c4cc-1795-459f-e152-703e9cd9d306"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True, True, True, True, True, True, True, True, True,\n",
              "          True, True, True]]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor[-1]  # last token of the input_tensor is the EOS token, 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAdgvkAEPzh2",
        "outputId": "62471c2f-fef7-4947-dc66-3f0db8b12803"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input = torch.tensor([[SOS_token]], device = device)\n",
        "decoder_hidden = encoder_second_output"
      ],
      "metadata": {
        "id": "st_BI7zbVQzk"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_first_output, decoder_first_hidden = decoder (decoder_input, decoder_hidden)"
      ],
      "metadata": {
        "id": "FK9n9obKVbq9"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_first_output.shape, decoder_first_hidden.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zPyvO5sViLa",
        "outputId": "c9a1dc79-cefb-4112-f87a-6d854cae6a17"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 2802]), torch.Size([1, 1, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_first_output.topk(1) # top_value, top_index\n",
        "# torch.topk function in PyTorch is used to return the k largest (or smallest) elements of a tensor along a specified dimension."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-32fZ3-Wmqm",
        "outputId": "26d94845-af18-463e-aa2c-7e98defc8e49"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([[-7.4262]], grad_fn=<TopkBackward0>),\n",
              "indices=tensor([[2057]]))"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_value, top_index = decoder_first_output.topk(1)  # both tensors with shape ([1, 1])\n",
        "top_index, top_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwSrejIJXEtK",
        "outputId": "84615ac1-6b13-43d0-c808-f194d6715330"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[2057]]), torch.Size([1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_index.squeeze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZgheOfgXPIJ",
        "outputId": "1ab45aaf-257e-4396-9ead-f8e8aa7270f9"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2057)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_index.squeeze().detach()\n",
        "# .detach(): This operation creates a tensor that shares storage with topi but without tracking the operations in the computation graph.\n",
        "# This is important because, during the decoding process, you don't want to backpropagate gradients through the sampled output (in this case, topi)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPC-zAlKXSU4",
        "outputId": "f4f69423-256d-40d4-82e9-0b6446460da8"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2057)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teaching_forcing_ratio = 0.2\n",
        "r = random.random()\n",
        "use_teaching_forcing = r > teaching_forcing_ratio\n",
        "r, use_teaching_forcing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJmPgsPGYHdX",
        "outputId": "c56c279a-fde9-4d41-a03e-575881c4bdac"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.15989888831532872, False)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_input, decoder_input.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ToD21SiY0YY",
        "outputId": "62c841e5-6c35-42ff-cfb4-d1517fdcdb48"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0]]), 0)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance evaluation on the test set"
      ],
      "metadata": {
        "id": "J7_aaOH7S73x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nh9oCwt1TBKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}